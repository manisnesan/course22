{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c88129b",
   "metadata": {},
   "source": [
    "# The forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a803f",
   "metadata": {},
   "source": [
    "## High Level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79dc46a",
   "metadata": {},
   "source": [
    "- [x] Using mnist dataset, create a basic architecture \n",
    "  - two linear layers with relu inbetween\n",
    "- [x] Create a loss function for multiclass (for simplicity use MSE)\n",
    "- [x] Forward pass passing the inputs and computing the loss using output\n",
    "- [x] Backward pass - computing the grads of out and mult with previous layer inp due to the chain layer (dy/dx = dy/du * du/dx). This is done in order to computer the grads of out wrt to the inp.\n",
    "- Refactor : Convert the individual layers as classes\n",
    "- Refactor : Abstract the repetitive code into base class\n",
    "- Using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close, test_eq\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap']='gray'\n",
    "torch.set_printoptions(precision=2,linewidth=125,sci_mode=False)\n",
    "np.set_printoptions(precision=2,linewidth=125)\n",
    "\n",
    "path_data=Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train,y_train),(x_valid,y_valid),_) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28daa4a9",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15498495",
   "metadata": {},
   "source": [
    "### Basic architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cca954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "n, m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6228dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden - arbitrary\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59889b66",
   "metadata": {},
   "source": [
    "50000 rows X 784 cols @ 784 rows X 10 cols -> 50000 images X 10 \n",
    "First row is the pixel val of the image.\n",
    "\n",
    "We are going to do this in two steps with hidden layers\n",
    "\n",
    "50000 X 784 @ 784 X 50 @ 50 X 10\n",
    "\n",
    "Weight matrix is initiallized with random values. We will also need to add bias that's what makes it linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m,nh)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh, 1) # we are going to create only 1 output(what number it is) inorder to use MSE instead of Cross Entroy\n",
    "b2 = torch.zeros(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e474c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5b959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 50])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lin(x_valid, w1, b1);t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aaddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tensor(-5.5).clamp_min(0.), tensor(0.))\n",
    "test_eq(tensor(5.5).clamp_min(0.), tensor(5.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d6dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n",
       "        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n",
       "        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n",
       "        ...,\n",
       "        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n",
       "        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n",
       "        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=relu(t);t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20135a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202502f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(x_valid);res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf9a13",
   "metadata": {},
   "source": [
    "### Loss function: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390718e4",
   "metadata": {},
   "source": [
    "To keep things simple, we are using mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d9b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1]), torch.Size([10000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da11af",
   "metadata": {},
   "source": [
    "1 will be broadcasted over 10000 elem, moves over to the previous since there are no axis, inserts a unit axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4615c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res - y_valid).shape # this is not what we want to compute mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5635c7",
   "metadata": {},
   "source": [
    "We need to get rid of the trailing (,1) in order to use mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed081777",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(res[:, 0].shape, torch.Size([10000]))\n",
    "test_eq(res.squeeze().shape, torch.Size([10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c609f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res.squeeze() - y_valid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bfcb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1]), torch.Size([50000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_valid = y_train.float(), y_valid.float()\n",
    "preds = model(x_train)\n",
    "preds.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebc6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targ): return (output.squeeze() - targ).pow(2).mean() # error squared mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91feeabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4308.76)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6a00f",
   "metadata": {},
   "source": [
    "### Gradients and backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71112c8f",
   "metadata": {},
   "source": [
    "Gradients are slope ie tangent at every point to the curve.\n",
    "Rise/run -> as we increase in time, how much the distance will increase.\n",
    "\n",
    "As we increase the weight, how much loss would go down. Derivative of the loss wrt to the weight will inform us how much to change the weight. SGD is New W -> W - dL/dw. \n",
    "Req - Loss should be a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sympy -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57faa11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x$"
      ],
      "text/plain": [
       "2*x"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols,diff\n",
    "x,y = symbols('x y')\n",
    "diff(x**2, x) # diff x^2 wrt x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebef309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 6 x$"
      ],
      "text/plain": [
       "6*x"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(3*x**2+9, x) # # diff 3x^2 + 9 wrt x -> Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2182e",
   "metadata": {},
   "source": [
    "Chain rule visualized - [The Intuitive Notion of the Chain Rule](https://webspace.ship.edu/msrenault/geogebracalculus/derivative_intuitive_chain_rule.html)\n",
    "\n",
    "Start at the end - takes it derivative & mult with the previous step-> backpropagation using the chain rule\n",
    "```\n",
    "L(l2, y)\n",
    "l2(r, w2, b2)\n",
    "r(l1)\n",
    "l1(x, w1, b1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab09e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 1]), torch.Size([1, 50]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.shape, w2.t().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6439798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1]), torch.Size([50000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2945a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000]), torch.Size([50000]), torch.Size([50000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:, 0].shape, preds.squeeze(dim=1).shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79415e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1]), torch.Size([50000, 784]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((preds[:, 0] - y_train)[:, None]).shape, x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8988e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input # dy/dx = dy/du * du/dx\n",
    "    # below comments are for the lin_grad(l2, out, w2, b2)\n",
    "    inp.g = out.g @ w.t() # chain rule -> grad of o/p matmul with transponse of weights -> (50000, 50) -> (50000, 1) * (1, 50)\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0) # w.g = inp.T()@out.g - (50, 1) -> (50000, 50, 1) * (50000, 1, 1) \n",
    "    b.g = out.g.sum(0) # -> grad of output added together (1) -> (50000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward\n",
    "    l1 = lin(inp, w1, b1) # [50000, 50] -> ([50000, 784]), ([784, 50]), ([50])\n",
    "    l2 = relu(l1) # [50000, 50]\n",
    "    out = lin(l2, w2, b2)  # ([50000, 1])\n",
    "    diff = out[:, 0] - targ # ([50000])\n",
    "    loss = diff.pow(2).mean() # scalar\n",
    "    \n",
    "    # backward - store the grad of each layer with its input in an attr 'g'\n",
    "    # import pdb; pdb.set_trace()\n",
    "    out.g = 2 * diff[:,None] / inp.shape[0]  # (50000, 1) -> (50000, 1) / 50000 (mean)\n",
    "    lin_grad(l2, out, w2, b2) # mult by the grad of previous layer\n",
    "    l1.g = (l1>0).float() * l2.g # (50000, 50) -> (50000, 50)\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a991f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for testing against later\n",
    "def get_grad(o): return o.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81afb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = w1g,w2g,b1g,b2g,ig = map(get_grad, chks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ace0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch \n",
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = map(mkgrad, chks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    l1 = lin(inp, w12, b12)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w22, b22)\n",
    "    return mse(out,targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424fa430",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e47ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(grads, ptgrads): test_close(a.grad, b, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c517f",
   "metadata": {},
   "source": [
    "## Refactor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe1bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = lin(inp, self.w, self.b)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp,self.targ = inp,targ\n",
    "        self.out = mse(inp, targ)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1224620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fcf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)\n",
    "loss = model(x_train, y_train)\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1851167",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5385a2c",
   "metadata": {},
   "source": [
    "### Module.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "    def bwd(self): raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5636e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)\n",
    "loss = model(x_train, y_train)\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304a635",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in,n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "    def forward(self, inp): return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee170f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return F.mse_loss(x, targ[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e664545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 50)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd86fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ded3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n",
       "          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n",
       "        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n",
       "        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2dbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
